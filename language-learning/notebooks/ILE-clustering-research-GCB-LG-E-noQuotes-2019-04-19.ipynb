{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILE clustering: no clusters with high frequency words?\n",
    "\n",
    "**Explain or discuss the fact that ILE cluster formation takes place only with MWC=1** -- \n",
    "[UPP Project plan](https://docs.google.com/spreadsheets/d/1TPbtGrqZ7saUHhOIi5yYmQ9c-cvVlAGqY14ATMPVCq4/edit#gid=624274537&range=C175).\n",
    "\n",
    "\"Gutenberg Children Books\" corpus, new \"LG-E-noQuotes\" dataset (GC_LGEnglish_noQuotes_fullyParsed.ull),  \n",
    "trash filter off: `min_word_count = 1`; `max_sentence_length` off; Link Grammar 5.5.1.  \n",
    "\n",
    "This notebook is shared as static [ILE-clustering-research-GCB-LG-E-noQuotes-2019-04-19.html](http://langlearn.singularitynet.io/data/clustering_2019/html/ILE-clustering-research-GCB-LG-E-noQuotes-2019-04-19.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:11.617645Z",
     "start_time": "2019-04-20T13:12:10.923825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-20 13:12:11 UTC :: module_path: /home/obaskov/94/ULL\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)\n",
    "from src.grammar_learner.utl import UTC, kwa, test_stats\n",
    "from src.grammar_learner.read_files import check_dir, check_mst_files\n",
    "from src.grammar_learner.write_files import list2file\n",
    "from src.grammar_learner.widgets import html_table\n",
    "from src.grammar_learner.preprocessing import filter_links\n",
    "tmpath = module_path + '/tmp/'; check_dir(tmpath, True, 'none')\n",
    "print(UTC(), ':: module_path:', module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus test settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:12.035277Z",
     "start_time": "2019-04-20T13:12:12.022486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-20 13:12:12 UTC \n",
      " /home/obaskov/94/ULL/output/ILE-clustering-GCB-LG-E-noQuotes-2019-04-20\n"
     ]
    }
   ],
   "source": [
    "corpus = 'GCB' # 'Gutenberg-Children-Books-Caps' \n",
    "dataset = 'LG-E-noQuotes'\n",
    "input_parses = module_path + '/data/GCB/LG-E-noQuotes/'\n",
    "kwargs = {\n",
    "    'left_wall'     :   ''          ,\n",
    "    'period'        :   False       ,\n",
    "    'context'       :   2           ,\n",
    "    'min_word_count':   1           ,\n",
    "    'word_space'    :   'discrete'  ,\n",
    "    'clustering'    :   'group'     ,\n",
    "    'cluster_range' :   [0]         ,\n",
    "    'top_level'     :   0.01        ,\n",
    "    'grammar_rules' :   2           ,\n",
    "    'max_disjuncts' :   1000000     ,\n",
    "    'stop_words'    :   []          ,\n",
    "    'tmpath'        :   tmpath      ,\n",
    "    'verbose'       :   '+'      ,\n",
    "    'template_path' :   'poc-turtle',\n",
    "    'linkage_limit' :   1000        }\n",
    "rp = module_path + '/data/' + corpus + '/LG-E-noQuotes'\n",
    "cp = rp  # corpus path = reference_path\n",
    "out_dir = module_path + '/output/' \\\n",
    "    + 'ILE-clustering-GCB-LG-E-noQuotes-' + str(UTC())[:10]\n",
    "kwargs['output_grammar'] = out_dir\n",
    "check_dir(out_dir, True)\n",
    "print(UTC(), '\\n', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parses ⇒ links ⇒ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:12.200155Z",
     "start_time": "2019-04-20T13:12:12.038514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/obaskov/94/ULL/data/GCB/LG-E-noQuotes/GC_LGEnglish_noQuotes_fullyParsed.ull']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files, re01 = check_mst_files(input_parses, 'max')\n",
    "kwargs['input_files'] = files; files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:09:07.297475Z",
     "start_time": "2019-04-20T08:09:07.292528Z"
    }
   },
   "source": [
    "## Extract links (pairs of linked words) from parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:33.689310Z",
     "start_time": "2019-04-20T13:12:12.203103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433259 unique links (pairs of linked words in parses)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>link</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>and+</td>\n",
       "      <td>6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>but+</td>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>was+</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not</td>\n",
       "      <td>did-</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>and+</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  link  count\n",
       "0    ,  and+   6601\n",
       "1    ,  but+   1471\n",
       "2   it  was+   1405\n",
       "3  not  did-    822\n",
       "4   he  and+    766"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links, re02 = filter_links(files, **kwargs)\n",
    "print(len(links), 'unique links (pairs of linked words in parses)')\n",
    "links[['word', 'link', 'count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:08:17.846735Z",
     "start_time": "2019-04-20T08:08:17.804571Z"
    }
   },
   "source": [
    "## Extract words from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:33.770426Z",
     "start_time": "2019-04-20T13:12:33.690980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22067 unique words in links (pairs of linked words in parses)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'d</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  count\n",
       "0    !    190\n",
       "1    $      6\n",
       "2    &      6\n",
       "3    '    193\n",
       "4   'd    339"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = links[['word', 'count']].groupby('word').agg({'count': 'sum'}).reset_index()\n",
    "print(len(words), 'unique words in links (pairs of linked words in parses)')\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:03:16.416037Z",
     "start_time": "2019-04-20T08:03:16.390506Z"
    }
   },
   "source": [
    "## Word counts: dict: `{word: number_of_observations}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:33.873437Z",
     "start_time": "2019-04-20T13:12:33.771916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22067 words total,\n",
      " 8614 words observed only once,\n",
      " 3271 words observed twice,\n",
      " 10182 words observed more than twice\n"
     ]
    }
   ],
   "source": [
    "word_counts = words.set_index('word').to_dict()['count']\n",
    "print(len(word_counts), 'words total,\\n',\n",
    "      len([w for w,c in word_counts.items() if c < 2]), 'words observed only once,\\n', \n",
    "      len([w for w,c in word_counts.items() if c == 2]), 'words observed twice,\\n', \n",
    "      len([w for w,c in word_counts.items() if c > 2]), 'words observed more than twice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links ⇒ disjuncts ⇒ clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T08:00:09.209161Z",
     "start_time": "2019-04-20T08:00:09.195769Z"
    }
   },
   "source": [
    "## DataFrame: word - link - number of observations in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:49.849255Z",
     "start_time": "2019-04-20T13:12:33.875590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>disjuncts</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[!]</td>\n",
       "      <td>(!-, !- &amp; crack+ &amp; !+, !- &amp; musket-shots+ &amp; fl...</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[$]</td>\n",
       "      <td>(price- &amp; 1.25+, price- &amp; 1.50+)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[&amp;]</td>\n",
       "      <td>(grosset- &amp; dunlap+, marshall- &amp; company+, mar...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[']</td>\n",
       "      <td>(,- &amp; struggles- &amp; gainst+, an- &amp; sheep-bells+...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['d]</td>\n",
       "      <td>(anybody- &amp; have+, emily- &amp; been+, he- &amp; and+,...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words                                          disjuncts  count\n",
       "0   [!]  (!-, !- & crack+ & !+, !- & musket-shots+ & fl...    190\n",
       "1   [$]                   (price- & 1.25+, price- & 1.50+)      6\n",
       "2   [&]  (grosset- & dunlap+, marshall- & company+, mar...      6\n",
       "3   [']  (,- & struggles- & gainst+, an- & sheep-bells+...    193\n",
       "4  ['d]  (anybody- & have+, emily- & been+, he- & and+,...    339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = links[['word', 'link', 'count']].copy()\n",
    "df['disjuncts'] = [[x] for x in df['link']]\n",
    "del df['link']\n",
    "df = df.groupby('word').agg({'disjuncts': 'sum', 'count': 'sum'}).reset_index()\n",
    "df['words'] = [[x] for x in df['word']]\n",
    "del df['word']\n",
    "df['disjuncts'] = df['disjuncts'].apply(lambda x: tuple(sorted(x)))\n",
    "df[['words', 'disjuncts', 'count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disjuncts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:49.867226Z",
     "start_time": "2019-04-20T13:12:49.850608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18939 unique disjuncts\n"
     ]
    }
   ],
   "source": [
    "dj_list = df['disjuncts'].tolist()\n",
    "djset = set(df['disjuncts'].tolist())\n",
    "print(len(djset), 'unique disjuncts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.201704Z",
     "start_time": "2019-04-20T13:12:49.870088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18939 grammar rules after clustering disjuncts\n"
     ]
    }
   ],
   "source": [
    "rules = df[['words', 'disjuncts']].groupby('disjuncts')['words'].apply(sum) \\\n",
    "    .reset_index().copy().rename(columns = {'words': 'cluster_words'})\n",
    "print(len(rules), 'grammar rules after clustering disjuncts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.213199Z",
     "start_time": "2019-04-20T13:12:53.203099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_words</th>\n",
       "      <th>disjuncts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ahem]</td>\n",
       "      <td>(!+ &amp; grandma+,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[no]</td>\n",
       "      <td>(!+ &amp; one+, ,+ &amp; and+, ,+ &amp; anne+, ,+ &amp; beast+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[!]</td>\n",
       "      <td>(!-, !- &amp; crack+ &amp; !+, !- &amp; musket-shots+ &amp; fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[crack]</td>\n",
       "      <td>(!-, 's- &amp; a-, ability- &amp; to-, and- &amp; that- &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@number@]</td>\n",
       "      <td>(!-, 's- &amp; chain-, 's- &amp; daughter-, 's- &amp; fort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_words                                          disjuncts\n",
       "0        [ahem]                                   (!+ & grandma+,)\n",
       "1          [no]  (!+ & one+, ,+ & and+, ,+ & anne+, ,+ & beast+...\n",
       "2           [!]  (!-, !- & crack+ & !+, !- & musket-shots+ & fl...\n",
       "3       [crack]  (!-, 's- & a-, ability- & to-, and- & that- & ...\n",
       "4    [@number@]  (!-, 's- & chain-, 's- & daughter-, 's- & fort..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[['cluster_words', 'disjuncts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters longer than 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.346692Z",
     "start_time": "2019-04-20T13:12:53.214504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 clusters contain 2 or more words; 18278 \"clusters\" are single-word\n"
     ]
    }
   ],
   "source": [
    "cluster_list = [c for c in rules['cluster_words'].tolist() if len(c) > 1]\n",
    "print(len(cluster_list), 'clusters contain 2 or more words;', \n",
    "      len(rules) - len(cluster_list), '\"clusters\" are single-word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.487347Z",
     "start_time": "2019-04-20T13:12:53.348010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 12 clusters:\n",
      "\n",
      "['1.25', '1.50']\n",
      "['beckwiths', 'marooners']\n",
      "['balancin', 'blind-man', 'bumpsterhausen', 'ceylon', 'clarkman', 'ewald', 'georgie', 'gerald', 'gleeson', 'hobson', 'holloway', 'italy', 'janey', 'jeannie', 'luella', 'malley', 'matey', 'midge', 'minot', 'morison', 'netty', 'niagara', 'strutt', 'theer', 'toady', 'toff', 'tom-and-kate', 'ventnor', 'webster']\n",
      "['bruk', 'deadliest', 'forty-three', 'living-address', 'nemesis', 'occurrences', 'peculiarities', 'relict', 'riddles', 'setness', 'seventy-six']\n",
      "['up-wind', 'upwind']\n",
      "['aylmer', 'bask', 'cold-eyed', 'geneva', 'long-concealed', 'morrice-dancers', 'odour-freighted', 'paperarello', 'pistils', 'plaster-worker', 'postage-stamps', 'psychological', 'saddles', 'sniffle', 'treasons', 'volaterrae']\n",
      "['anguished', 'deep-set']\n",
      "['1871-72', 'anne-girl', 'apes', 'bev', 'bibles', 'blackberries', 'boot', 'bournemouth', 'bridles', 'bristling', 'carefree', 'centaurs', 'chillon', 'davie', 'depression', 'double-seated', 'dry-eyed', 'e.c', 'eb', 'ede', 'editeur', 'fishy-like', 'flecked', 'fleetly', 'germs', 'goldenly-glad', 'icelandic', 'isobel', 'langsyne', 'ld', 'litill', 'long-bearded', 'lubber', \"ma'am\", 'minion', 'misled', 'nets', 'panthers', 'plantain', 'privation', 'roofed', 'sar', 'scared-like', 'set-lipped', 'shrined', 'sirs', 'stags', 'stratagems', 'studded', 'talon', 'tamarinds', 'theatres', 'u.s.a.', 'unintimidated', 'virginal', 'vol', 'wilfy', 'winnie', 'yawning']\n",
      "['esme', 'millie']\n",
      "['brocades', 'cinnamon']\n",
      "['prey', 'purply-black']\n",
      "['holmes', 'langley']\n"
     ]
    }
   ],
   "source": [
    "print('Random 12 clusters:\\n')\n",
    "for c in cluster_list[:12]: print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.602689Z",
     "start_time": "2019-04-20T13:12:53.489970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes observed more than once:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Cluster size</td><td>Number of clusters</td></tr><tr><td>2</td><td>352</td></tr><tr><td>3</td><td>102</td></tr><tr><td>4</td><td>63</td></tr><tr><td>5</td><td>32</td></tr><tr><td>6</td><td>25</td></tr><tr><td>7</td><td>17</td></tr><tr><td>9</td><td>9</td></tr><tr><td>13</td><td>6</td></tr><tr><td>10</td><td>5</td></tr><tr><td>11</td><td>4</td></tr><tr><td>15</td><td>4</td></tr><tr><td>14</td><td>4</td></tr><tr><td>8</td><td>4</td></tr><tr><td>16</td><td>3</td></tr><tr><td>26</td><td>3</td></tr><tr><td>12</td><td>3</td></tr><tr><td>19</td><td>3</td></tr><tr><td>21</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_sizes = Counter([len(c) for c in cluster_list])\n",
    "print('Cluster sizes observed more than once:')\n",
    "display(html_table([['Cluster size', 'Number of clusters']] + \n",
    "                   sorted([[s,n] for s,n in cluster_sizes.items() if n > 1], \n",
    "                          key = lambda x : x[1], reverse = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.662237Z",
     "start_time": "2019-04-20T13:12:53.605724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes observed only once:\n",
      " [17, 20, 22, 25, 29, 34, 36, 38, 39, 40, 43, 49, 51, 54, 55, 59, 73, 162, 173, 417]\n"
     ]
    }
   ],
   "source": [
    "print('Cluster sizes observed only once:\\n', sorted([s for s,n in cluster_sizes.items() if n < 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.722643Z",
     "start_time": "2019-04-20T13:12:53.665616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3789 unique words in clusters -- 100 randomly chosen samples:\n",
      "\n",
      " ['well-shaped', 'nutting', 'jack-sparrow', 'minx', \"prigio's\", 'fameuse', 'setters', 'tapping', 'compass-needle', 'stags', 'hamper', 'heightened', 'queerer', 'nantes', 'nyamatsanes', 'neighing', 'piave', 'plodded', 'bishop', 'stuttered', 'conquests', 'steak', 'braved', 'unintimidated', 'fiercer', 'thirty-nine', 'resident', 'vinegar', 'light-heartedness', 'restful', 'wind-ruddy', 'toff', 'pathways', 'carney', 'feasible', 'realism', 'crowsfeet', 'outrageous', 'pook', 'lilac-bush', 'ill-health', 'quick-spread', 'county', 'horoscopes', 'klondike', 'speculative', 'daredevil', 'thomson', 'puny', 'bournemouth', 'carnations', 'notebooks', 'vol', 'corroding', 'interfering', 'kootenay', 'callender', 'fag-end', 'mirrored', 'testily', 'invader', 'lantern-light', 'majestic', 'aforethought', 'prop', 'picnicked', 'neatness', 'govor', 'crisply', 'sun-struck', 'prometheus', 'prescription', 'annual', 'lackadiasical', 'shopped', 'complied', 'half-drowned', \"ha'nted\", 'robe-edge', 'buckwheats', 'chrissie', 'kettley', 'workers', 'archives', 'gul-rukh', 'curtainless', 'gooseberries', 'stilled', 'bagdad', 'impracticable', 'chary', 'earth-stars', 'kilter', 'elopements', 'out-at-elbows', 'pianno', 'friendliness', 'afflictions', 'cedar', 'heaviest']\n"
     ]
    }
   ],
   "source": [
    "# unique words in clusters:\n",
    "words_in_clusters = set([w for c in cluster_list for w in c])\n",
    "print(len(words_in_clusters), 'unique words in clusters', \n",
    "      '-- 100 randomly chosen samples:\\n\\n', list(words_in_clusters)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.786684Z",
     "start_time": "2019-04-20T13:12:53.726015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numbers of cluster member words observations in the whole corpus\n",
    "wcs = set([word_counts[w] for w in words_in_clusters]); wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.849517Z",
     "start_time": "2019-04-20T13:12:53.789859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3416, 2: 289, 3: 49, 6: 9, 5: 11, 4: 12, 7: 1, 9: 2})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_word_counts = Counter([word_counts[w] for w in words_in_clusters])\n",
    "clustered_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_`1: 3416` means 3416 words represented in clusters are observed once in the corpus,  \n",
    "`2: 289` -- 289 words are observed twice, ... 2 most frequent clustered words are observed 9 times_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:53.958775Z",
     "start_time": "2019-04-20T13:12:53.852608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% words in clusters are observed only once in the whole input corpus:\n",
      " 3416 once observed words of 3789 total unique words in clusters.\n"
     ]
    }
   ],
   "source": [
    "print(str(int(round(clustered_word_counts[1]/len(words_in_clusters)*100,0))) + \n",
    "      '% words in clusters are observed only once in the whole input corpus:\\n',\n",
    "      clustered_word_counts[1], 'once observed words of', \n",
    "      len(words_in_clusters), 'total unique words in clusters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:54.054092Z",
     "start_time": "2019-04-20T13:12:53.962232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations of clustered words in the whole corpus for words observed more than twice:\n",
      "\n",
      " {'1.50': 5, 'marooners': 4, 'malley': 3, 'ventnor': 3, 'anguished': 3, 'deep-set': 4, 'editeur': 3, \"ma'am\": 9, 'esme': 6, 'leroux': 3, 'xi': 9, 'xiv': 6, 'xix': 6, 'xv': 6, 'xxiii': 6, 'xxiv': 6, 'pined': 3, 'fretted': 3, 'haughty': 3, 'moped': 4, 'curdken': 3, 'andy': 4, 'gestures': 3, 'lettered': 3, 'pest': 3, 'triumphs': 3, 'unprofitable': 3, 'tatters': 3, 'quizzically': 3, 'aloofness': 4, 'bread-and-butter': 3, 'lamented': 4, 'profoundly': 3, 'unnecessary': 3, 'unsuspicious': 3, 'incisive': 5, 'winning': 3, 'hotels': 3, 'juncture': 5, 'foolscap': 5, 'seven-league': 3, 'xxv': 5, 'xxvii': 3, 'xxviii': 3, 'xxx': 3, 'xxxi': 3, 'hazel-nut': 4, 'many-furred': 3, 'nut-brown': 3, 'blackest': 4, 'attendance': 3, 'january': 3, 'manitoba': 6, 'vancouver': 3, 'trice': 3, 'daytime': 7, 'grate': 3, 'bazar': 5, 'center': 3, 'shifty': 3, 'sealskin': 6, 'shaws': 3, 'thankfulness': 3, 'yore': 3, 'checkers': 3, 'spices': 5, 'sinclairs': 3, 'enderly': 5, 'petersen': 4, 'sagely': 3, 'stony': 3, 'intervening': 3, \"wendy's\": 3, 'easiest': 4, 'funniest': 3, 'handsomest': 5, 'kindest': 4, 'lowest': 5, 'merest': 4, 'merriest': 3, 'pleasantest': 5, 'quickest': 3, 'jolliest': 6, 'softest': 3}\n"
     ]
    }
   ],
   "source": [
    "clustered_words_counts = {w: word_counts[w] for c in cluster_list for w in c}\n",
    "frequent_words_counts = {w:c for w,c in clustered_words_counts.items() if c > 2}\n",
    "print('Number of observations of clustered words in the whole corpus',\n",
    "      'for words observed more than twice:\\n\\n', frequent_words_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:07:57.781239Z",
     "start_time": "2019-04-19T16:07:57.732669Z"
    }
   },
   "source": [
    "## Clustering patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:54.129084Z",
     "start_time": "2019-04-20T13:12:54.057438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(1, 5): 3,\n",
       "         (1, 4): 3,\n",
       "         (1, 2, 3): 6,\n",
       "         (1,): 478,\n",
       "         (1, 2): 120,\n",
       "         (3, 4): 1,\n",
       "         (1, 2, 3, 9): 1,\n",
       "         (2, 6): 1,\n",
       "         (2,): 14,\n",
       "         (1, 3): 8,\n",
       "         (6, 9): 1,\n",
       "         (2, 3, 4): 2,\n",
       "         (2, 3): 9,\n",
       "         (1, 2, 3, 4): 1,\n",
       "         (2, 4): 2,\n",
       "         (3,): 1,\n",
       "         (3, 5): 1,\n",
       "         (1, 2, 5): 1,\n",
       "         (1, 2, 3, 5): 1,\n",
       "         (2, 3, 6): 1,\n",
       "         (1, 2, 3, 7): 1,\n",
       "         (2, 5): 2,\n",
       "         (1, 6): 1,\n",
       "         (1, 2, 3, 4, 5): 1,\n",
       "         (3, 6): 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = Counter([tuple(sorted(set([word_counts[w] for w in c]))) for c in cluster_list])\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:08:59.535221Z",
     "start_time": "2019-04-19T16:08:59.529974Z"
    }
   },
   "source": [
    "_Comment: `(1,5): 3` means that 3 clusters consist of words observed in the input corpus once or 5 times_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters of words observed more than once in the input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:54.198785Z",
     "start_time": "2019-04-20T13:12:54.132092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587 clusters of 661 consist of words, observed only once in the input corpus,\n",
      " 74 clusters of words, observed more than once in the input corpus:\n",
      "['bumpsterhausen', 'clarkman', 'holloway', 'malley', 'toady', 'ventnor']\n",
      "['anguished', 'deep-set']\n",
      "['editeur', \"ma'am\", 'roofed']\n",
      "['esme', 'millie']\n",
      "['brocades', 'cinnamon']\n",
      "['p64.jpg', 'p99.jpg']\n",
      "['xi', 'xiv', 'xix', 'xv', 'xxiii', 'xxiv']\n",
      "['milkmaid', 'scraper']\n",
      "['classics', 'jingling', 'jukes', 'oyster-shops', 'pined', 'railroad-hacks', 'stocks', 'tegumai', 'turley', '|marilla']\n",
      "['....n', 'clucked', 'creaked', 'dukes', 'fretted', 'haughty', 'matured', 'moped', 'threshed', 'twittered']\n",
      "['homeless', 'shingled']\n",
      "['derision', 'mogarzea', 'wattle']\n",
      "['curdken', 'untrue']\n",
      "['andy', 'attributes', 'chow-chow', 'clambered', 'commented', 'crackling', 'fiddled', 'fume', 'gestures', 'lettered', 'magog', 'magsie', 'mature', 'militza', 'omnibuses', 'pathways', 'pest', 'precipitated', 'scheming', 'stilled', 'sue', 'top-heavy', 'triumphs', 'unprofitable']\n",
      "['fisher-folk', 'praises']\n",
      "['delirium', 'peshawur', 'seclusion', 'tatters', 'temperament']\n",
      "['fiercer', 'piercingly', 'quizzically']\n",
      "['aloofness', 'bread-and-butter', 'camels']\n",
      "['lamented', 'snowshoeing']\n",
      "['disdainfully', 'pleadingly']\n",
      "['girlishly', 'profoundly']\n",
      "['unnecessary', 'unsuspicious']\n",
      "['fidgety', 'obedient']\n",
      "['incisive', 'winning']\n",
      "['bombay', 'mhow', 'nucklao', 'wish-ton-wish']\n",
      "['boat-side', 'hotels']\n",
      "['beauty-spots', 'dangerously']\n",
      "['quixotic', 'totting']\n",
      "['fidget', 'leak']\n",
      "['foolscap', 'leather-bound']\n",
      "['inlaid', 'innermost']\n",
      "['assault', 'contraries', 'degrees', 'proxy', 'waltzing']\n",
      "['xxv', 'xxvii', 'xxviii', 'xxx', 'xxxi', 'xxxii']\n",
      "['many-furred', 'poor-spirited']\n",
      "['bulging', 'horror-stricken']\n",
      "['dark-brown', 'nut-brown']\n",
      "['cloverside', 'heartsease']\n",
      "['buda', 'luncheon']\n",
      "['ragged-looking', 'snippy']\n",
      "['iron-gray', 'wavy']\n",
      "['blackest', 'whitest']\n",
      "['attendance', 'denial', 'midstream', 'supplication', 'winnipeg']\n",
      "['anatomy', 'hopelessness', 'january', 'norroway', 'switzerland']\n",
      "['campden', 'fractions', 'manitoba', 'vancouver']\n",
      "['daytime', 'grate', 'lurch']\n",
      "['bazar', 'turret-room']\n",
      "['center', 'mellowness']\n",
      "['goldfish', 'spinning-top', 'traitor']\n",
      "['gold-bearded', 'grey-haired', 'white-haired']\n",
      "['braddon', 'brompton', 'carney']\n",
      "['birch-bark', 'blefuscu', 'borkum', 'hamel', 'koumongoé', 'lehon', 'mesopotamia', 'mugger-ghaut', 'pinks', 'shaws', 'thankfulness', 'yore']\n",
      "['checkers', 'croquet']\n",
      "['civilization', 'khanhiwara', 'missions', 'navarre', 'taram-tāq', 'wales']\n",
      "['brandy', 'impunity', 'shields', 'spices', 'unconcern']\n",
      "['countryside', 'indies']\n",
      "['arches', 'sinclairs', 'spy-glass']\n",
      "['caucasus', 'gruagach']\n",
      "['dial', 'divan']\n",
      "['burnleys', 'gear', 'mary-annish']\n",
      "['brush-grown', 'brushy']\n",
      "['hospitably', 'rock-firm']\n",
      "['bitterest', 'boldest', 'driest', 'easiest', 'fattest', 'fullest', 'funniest', 'grimmest', 'handsomest', 'kindest', 'laziest', 'lowest', 'merest', 'merriest', 'oddest', 'pleasantest', 'quickest', 'thinnest', 'tiniest', 'ugliest']\n",
      "['jolliest', 'softest']\n",
      "['penningtons', 'spencers']\n",
      "['allans', 'verdure']\n",
      "['garden-engine', 'lateness']\n",
      "['greece', 'hack', 'marsden', 'procrastinate', 'saharunpore']\n",
      "['byre', 'treadmill']\n",
      "['bath-time', 'dense', 'gliding', 'twenty-eight']\n",
      "['nugget', 'perambulator']\n",
      "['bothering', 'shunning']\n",
      "['chippings', 'trifled']\n",
      "['gusto', 'honker']\n",
      "['door-mat', 'water-butt']\n"
     ]
    }
   ],
   "source": [
    "once_observed_words = {w:c for w,c in word_counts.items() if c < 2}.keys()\n",
    "filtered_cluster_list = [l for l in [[w for w in c if w not in once_observed_words] \n",
    "                         for c in cluster_list] if len(l) > 1]\n",
    "print(len(cluster_list) - len(filtered_cluster_list),\n",
    "      'clusters of', len(cluster_list), \n",
    "      'consist of words, observed only once in the input corpus,\\n',\n",
    "      len(filtered_cluster_list),\n",
    "      'clusters of words, observed more than once in the input corpus:')\n",
    "for l in filtered_cluster_list: print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters of words observed more than twice in the input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:54.268472Z",
     "start_time": "2019-04-20T13:12:54.202063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 clusters of words, observed more than twice in the input corpus:\n",
      "\n",
      "['malley', 'ventnor']\n",
      "['anguished', 'deep-set']\n",
      "['editeur', \"ma'am\"]\n",
      "['xi', 'xiv', 'xix', 'xv', 'xxiii', 'xxiv']\n",
      "['fretted', 'haughty', 'moped']\n",
      "['andy', 'gestures', 'lettered', 'pest', 'triumphs', 'unprofitable']\n",
      "['aloofness', 'bread-and-butter']\n",
      "['unnecessary', 'unsuspicious']\n",
      "['incisive', 'winning']\n",
      "['xxv', 'xxvii', 'xxviii', 'xxx', 'xxxi']\n",
      "['manitoba', 'vancouver']\n",
      "['daytime', 'grate']\n",
      "['shaws', 'thankfulness', 'yore']\n",
      "['easiest', 'funniest', 'handsomest', 'kindest', 'lowest', 'merest', 'merriest', 'pleasantest', 'quickest']\n",
      "['jolliest', 'softest']\n"
     ]
    }
   ],
   "source": [
    "less_observed_words = {w:c for w,c in word_counts.items() if c < 3}.keys()\n",
    "filtered_cluster_list = [l for l in [[w for w in c if w not in less_observed_words] \n",
    "                         for c in cluster_list] if len(l) > 1]\n",
    "print(len(filtered_cluster_list),\n",
    "      'clusters of words, observed more than twice in the input corpus:\\n')\n",
    "for l in filtered_cluster_list: print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T10:06:48.564491Z",
     "start_time": "2019-04-20T10:06:48.560909Z"
    }
   },
   "source": [
    "## Clusters of words observed more than 3x in the input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T13:12:54.346823Z",
     "start_time": "2019-04-20T13:12:54.271671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clusters of words, observed more than 3x in the input corpus:\n",
      "\n",
      "['xi', 'xiv', 'xix', 'xv', 'xxiii', 'xxiv']\n",
      "['easiest', 'handsomest', 'kindest', 'lowest', 'merest', 'pleasantest']\n"
     ]
    }
   ],
   "source": [
    "less_observed_words = {w:c for w,c in word_counts.items() if c < 4}.keys()\n",
    "filtered_cluster_list = [l for l in [[w for w in c if w not in less_observed_words] \n",
    "                         for c in cluster_list] if len(l) > 1]\n",
    "print(len(filtered_cluster_list),\n",
    "      'clusters of words, observed more than 3x in the input corpus:\\n')\n",
    "for l in filtered_cluster_list: print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights:\n",
    "\n",
    "Unfiltered \"Gutenberg Children Books\" corpus \"LG-E-noQuotes\" dataset contains  \n",
    "22067 words, of which:\n",
    "- 8614 words are observed only once in the whole (unfiltered) corpus,  \n",
    "- 3271 words observed twice,\n",
    "- 10182 words observed more than twice (46% of the corpus).\n",
    "\n",
    "\"Identical Lexical Entries\" clustering provides 18939 grammar rules (clusters) containg  \n",
    "3789 unique words, 3416 (90%) of which are observed only once in the corpus. \n",
    "- 18278 \"clusters\" are single-word,\n",
    "- 661 clusters contain 2 or more words:\n",
    "    - 587 clusters consist of words, observed only once in the input corpus,\n",
    "    - 74 clusters of words, observed more than once,\n",
    "    - 2 clusters of words, observed more than 3x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
